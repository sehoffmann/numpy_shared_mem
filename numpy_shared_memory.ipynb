{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18936708",
   "metadata": {},
   "source": [
    "# Faster transfers via torch.Tensor shared memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1594a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr1 Size: 1MB\n",
      "Arr2 Size: 10MB\n",
      "Arr3 Size: 105MB\n",
      "Arr4 Size: 524MB\n",
      "Arr5 Size: 1049MB\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "def benchmark(func, n=10):\n",
    "    times = []\n",
    "    for _ in range(n):\n",
    "        ts1 = time.time()\n",
    "        func()\n",
    "        times.append(time.time() - ts1)\n",
    "    return float(np.mean(times)), float(np.std(times))\n",
    "\n",
    "arrays = [np.random.normal(size=(N, 128, 128, 8)) for N in [1, 10, 100, 500, 1000]]\n",
    "for i, arr in enumerate(arrays):\n",
    "    print(f'Arr{i+1} Size: {arr.nbytes/1000/1000:.0f}MB')\n",
    "\n",
    "\n",
    "def local_roundtrip(send_queue, arr):\n",
    "    send_queue.put(arr)\n",
    "    out = send_queue.get()\n",
    "    assert(arr[0,0,0,0] == out[0,0,0,0])\n",
    "    \n",
    "def remote_roundtrip(send_queue, recv_queue, arr):\n",
    "    send_queue.put(arr)\n",
    "    out = recv_queue.get()\n",
    "    assert(arr[0,0,0,0] == out[0,0,0,0])\n",
    "\n",
    "def worker_fn(send_queue, recv_queue):\n",
    "    while True:\n",
    "        arr = send_queue.get()\n",
    "        if arr is False:\n",
    "            break\n",
    "        recv_queue.put(arr)\n",
    "\n",
    "def create_process(send_queue, recv_queue):\n",
    "    p = mp.Process(target=worker_fn, args=(send_queue, recv_queue))\n",
    "    p.start()\n",
    "    return p\n",
    "\n",
    "def join_process(send_queue, p):\n",
    "    send_queue.put(False)\n",
    "    p.join()\n",
    "    \n",
    "def benchmark_local():\n",
    "    for i, arr in enumerate(arrays):\n",
    "        send_queue = mp.Queue()\n",
    "        mean,std = benchmark(lambda: local_roundtrip(send_queue, arr))\n",
    "        print(f'Arr{i+1}: {1000*mean:.0f}ms +- {1000*std:.0f}')\n",
    "\n",
    "def benchmark_remote():\n",
    "    for i, arr in enumerate(arrays):\n",
    "        send_queue = mp.Queue()\n",
    "        recv_queue = mp.Queue()\n",
    "        p = create_process(send_queue, recv_queue)\n",
    "        mean,std = benchmark(lambda: remote_roundtrip(send_queue, recv_queue, arr))\n",
    "        print(f'Arr{i+1}: {1000*mean:.0f}ms +- {1000*std:.0f}')\n",
    "        join_process(send_queue, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61371d64",
   "metadata": {},
   "source": [
    "### Current serialization performance (local roundtrip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3955c390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr1: 2ms +- 1\n",
      "Arr2: 26ms +- 4\n",
      "Arr3: 433ms +- 3\n",
      "Arr4: 2294ms +- 79\n",
      "Arr5: 4575ms +- 97\n"
     ]
    }
   ],
   "source": [
    "benchmark_local()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a131ff7",
   "metadata": {},
   "source": [
    "### Current serialization performance (remote roundtrip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d98b39dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr1: 3ms +- 3\n",
      "Arr2: 59ms +- 5\n",
      "Arr3: 872ms +- 10\n",
      "Arr4: 4487ms +- 113\n",
      "Arr5: 8935ms +- 148\n"
     ]
    }
   ],
   "source": [
    "benchmark_remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269b328",
   "metadata": {},
   "source": [
    "## Proposed Solution (Proof of Concept)\n",
    "\n",
    "**It's important that we recreate any Queues afterward for our new reduction method to be registered**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c826bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.reduction import ForkingPickler\n",
    "\n",
    "def rebuild_ndarray(tensor, dtype):\n",
    "    return tensor.numpy().view(dtype)\n",
    "\n",
    "def reduce_ndarray(arr):\n",
    "    tensor = torch.as_tensor(arr.view(np.int8))  # always interpret as raw bytes to support stuff like np.datetime64 as well\n",
    "    return (rebuild_ndarray, (tensor, arr.dtype))\n",
    "\n",
    "ForkingPickler.register(np.ndarray, reduce_ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa33c8",
   "metadata": {},
   "source": [
    "### Proposed Performance (local roundtrip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ffa0b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr1: 1ms +- 1\n",
      "Arr2: 4ms +- 1\n",
      "Arr3: 47ms +- 4\n",
      "Arr4: 242ms +- 8\n",
      "Arr5: 435ms +- 22\n"
     ]
    }
   ],
   "source": [
    "benchmark_local()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de672d3",
   "metadata": {},
   "source": [
    "### Current serialization performance (local roundtrip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba83d3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr1: 15ms +- 18\n",
      "Arr2: 23ms +- 15\n",
      "Arr3: 189ms +- 19\n",
      "Arr4: 565ms +- 34\n",
      "Arr5: 940ms +- 32\n"
     ]
    }
   ],
   "source": [
    "benchmark_remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546e790",
   "metadata": {},
   "source": [
    "## Actually sharing memory\n",
    "\n",
    "Notice that the current solution **always** results in a copy to shared memory when pickling. While this significantly speeds up transmissions, it still results in duplicated memory usage (assuming that the same array is either kept in the main process or shared across multiple workers).\n",
    "\n",
    "While we can easily create a numpy array that is a view to a torch.Tensor in shared memory (`arr = torch.as_tensor(arr).share_memory_().numpy()`), when serializing such an array, despite being already in shared memory, we would copy it again. This is because torch only looks at the storage type of a Tensor to determine if its already in shared memory and not at the actual address.\n",
    "\n",
    "Luckily for us, torch and numpy keep track of who actually owns the memory and thus `np.ndarray.base` will point to the original `torch.Tensor` on which we called `.numpy()`. Unfortunately for us, in case of slices, there is one more indirection.\n",
    "\n",
    "The following code demonstrates this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55601afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_ndarray(tensor, metainfo):\n",
    "    offset, shape, strides, typestr = metainfo\n",
    "    buffer = tensor.numpy()\n",
    "    return np.ndarray(buffer=buffer,offset=offset, shape=shape, strides=strides, dtype=typestr)\n",
    "\n",
    "def reduce_ndarray(arr: np.ndarray):\n",
    "    shape = arr.__array_interface__['shape']\n",
    "    strides = arr.__array_interface__['strides']\n",
    "    typestr = arr.__array_interface__['typestr']\n",
    "    \n",
    "    base = arr.base\n",
    "    while type(base) is np.ndarray and base.base is not None:  # only support pure np.ndarray's for now\n",
    "        base = base.base\n",
    "\n",
    "    if isinstance(base, torch.Tensor):\n",
    "        tensor = base\n",
    "        offset = np.asarray(base).__array_interface__['data'][0] - arr.__array_interface__['data'][0]\n",
    "    else:\n",
    "        tensor = torch.as_tensor(arr.view(np.int8))\n",
    "        offset = 0\n",
    "    \n",
    "    return (rebuild_ndarray, (tensor, (offset,shape,strides,typestr)))\n",
    "\n",
    "\n",
    "def share_memory(arr: np.ndarray) -> np.ndarray:\n",
    "    tensor = torch.as_tensor(arr.view(np.int8)).share_memory_()\n",
    "    return tensor.numpy()\n",
    "\n",
    "\n",
    "ForkingPickler.register(np.ndarray, reduce_ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede2fd7b",
   "metadata": {},
   "source": [
    "### Remote rountrip (assuming memory is already shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b06a953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr1: 4ms +- 8\n",
      "Arr2: 3ms +- 5\n",
      "Arr3: 3ms +- 6\n",
      "Arr4: 2ms +- 3\n",
      "Arr5: 2ms +- 3\n"
     ]
    }
   ],
   "source": [
    "for i, arr in enumerate(arrays):\n",
    "    send_queue = mp.Queue()\n",
    "    recv_queue = mp.Queue()\n",
    "    p = create_process(send_queue, recv_queue)\n",
    "    arr = share_memory(arr)\n",
    "    mean,std = benchmark(lambda: remote_roundtrip(send_queue, recv_queue, arr))\n",
    "    print(f'Arr{i+1}: {1000*mean:.0f}ms +- {1000*std:.0f}')\n",
    "    join_process(send_queue, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc34fe3",
   "metadata": {},
   "source": [
    "### Remote rountrip (sharing done during first serialization)\n",
    "-> Time is now /2 because we only need to copy to shared memory during the first serialization. The backtrip is for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f69171a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr1: 3ms +- 4\n",
      "Arr2: 8ms +- 2\n",
      "Arr3: 55ms +- 6\n",
      "Arr4: 256ms +- 23\n",
      "Arr5: 411ms +- 39\n"
     ]
    }
   ],
   "source": [
    "for i, arr in enumerate(arrays):\n",
    "    send_queue = mp.Queue()\n",
    "    recv_queue = mp.Queue()\n",
    "    p = create_process(send_queue, recv_queue)\n",
    "    mean,std = benchmark(lambda: remote_roundtrip(send_queue, recv_queue, arr))\n",
    "    print(f'Arr{i+1}: {1000*mean:.0f}ms +- {1000*std:.0f}')\n",
    "    join_process(send_queue, p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
